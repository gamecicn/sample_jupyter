{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP FP LSTM Emotional analysis_W2V .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNYtZr3UZfFxP9TfrWEvhhK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gamecicn/sample_jupyter/blob/main/NLP_FP_LSTM_Emotional_analysis_W2V_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asDAvqkwj4hV"
      },
      "source": [
        "# NLP FP Emotional analysis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAebpWgwkWCi",
        "outputId": "62760c04-6d09-425b-e0da-5a9582113f0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        }
      },
      "source": [
        "# Install\n",
        "!pip install numpy==1.16.2\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.16.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/d5/4f8410ac303e690144f0a0603c4b8fd3b986feb2749c435f7cdbb288f17e/numpy-1.16.2-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
            "\u001b[K     |████████████████████████████████| 17.3MB 173kB/s \n",
            "\u001b[31mERROR: umap-learn 0.4.6 has requirement numpy>=1.17, but you'll have numpy 1.16.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "Successfully installed numpy-1.16.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyRfktHBkPPE"
      },
      "source": [
        "# Setup\n",
        "\n",
        "\n",
        "\n",
        "# All the imports!\n",
        "\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "\n",
        "import tensorflow as tf \n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from keras.callbacks import LambdaCallback\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.models import Sequential\n",
        "\n",
        " \n",
        "# Supress deprecation warnings\n",
        "import logging\n",
        "logging.getLogger('tensorflow').disabled = True\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIKaXvVQk3br",
        "outputId": "8f64ff4e-27b3-4f23-ead1-92cb24ce99b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPHyAevFv3C1"
      },
      "source": [
        "## Sklearn import data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwzaemQLv1cB"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/My Drive/DS_data/ISEAR_aug.csv\", sep=\",\")\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "labels = df.emotion.factorize()\n",
        "labels_index = labels[1]\n",
        "df['emotion'] = labels[0]\n",
        "\n",
        "training_data, testing_data, y_train, y_test = train_test_split(df.text, df.emotion, test_size=0.1, random_state=123, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyF1OjoVzyt5",
        "outputId": "d0cea61d-e4fa-4896-b173-c3c97334cfa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "training_data.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4844     The teacher of one course gave me 10 questions...\n",
              "3130     I felt disgust with alcohol in general when a ...\n",
              "14152    When I was taking my girlfriend out, she seeme...\n",
              "9014     For example, to feel bad because of one's atti...\n",
              "12479    My friend replying to my letter and again sayi...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Y8CFJhx2Suf",
        "outputId": "e19bb18b-422e-47a1-de99-651d5e6250c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "y_train.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4844     4\n",
              "3130     4\n",
              "14152    4\n",
              "9014     5\n",
              "12479    0\n",
              "Name: emotion, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuMSCRm8ubXT"
      },
      "source": [
        "## Pre-processing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "en_zA75RDWeI"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdJqaAE1ufG3",
        "outputId": "30265e6d-85bc-4d9a-cf1a-4afc5b25d356",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "'''\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# extract features\n",
        "vectorizer1 = TfidfVectorizer(stop_words = \"english\")\n",
        "x_train = vectorizer1.fit_transform(training_data)\n",
        "# Use training data's vocabulary to create test tf-idf matrix\n",
        "vectorizer2 = TfidfVectorizer(stop_words = \"english\",vocabulary=vectorizer1.vocabulary_)\n",
        "x_test = vectorizer2.fit_transform(testing_data)\n",
        "\n",
        "vocab_size = x_train.shape[1]\n",
        "review_length = 500\n",
        "\n",
        "print(\"vocab_size : {}\".format(vocab_size))\n",
        "'''\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\n\\n# extract features\\nvectorizer1 = TfidfVectorizer(stop_words = \"english\")\\nx_train = vectorizer1.fit_transform(training_data)\\n# Use training data\\'s vocabulary to create test tf-idf matrix\\nvectorizer2 = TfidfVectorizer(stop_words = \"english\",vocabulary=vectorizer1.vocabulary_)\\nx_test = vectorizer2.fit_transform(testing_data)\\n\\nvocab_size = x_train.shape[1]\\nreview_length = 500\\n\\nprint(\"vocab_size : {}\".format(vocab_size))\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diHy2UmEDYgU"
      },
      "source": [
        "### Wrod2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wkhycd1KDbZV",
        "outputId": "b5646612-2412-471d-d634-598f9f40ed87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "import gensim\n",
        "\n",
        "print('\\nTraining word2vec...')\n",
        " \n",
        "\n",
        "max_sentence_len = max([ len(x) for x in df[\"text\"] ])\n",
        "\n",
        "word_model = gensim.models.Word2Vec(df[\"text\"], size=100, min_count=1, window=5, iter=100)\n",
        "pretrained_weights = word_model.wv.syn0\n",
        "vocab_size, emdedding_size = pretrained_weights.shape\n",
        "print('Result embedding shape:', pretrained_weights.shape)\n",
        "\n",
        "def word2idx(word):\n",
        "  return word_model.wv.vocab[word].index\n",
        "def idx2word(idx):\n",
        "  return word_model.wv.index2word[idx]\n",
        "\n",
        "print('\\nPreparing the data for LSTM...')\n",
        "\n",
        "x_train = np.zeros([len(training_data), max_sentence_len], dtype=np.int32)\n",
        "for i, sentence in enumerate(training_data):\n",
        "  for t, word in enumerate(sentence):\n",
        "    x_train[i, t] = word2idx(word)\n",
        "print('x_train shape:', x_train.shape)\n",
        "\n",
        "\n",
        "x_test = np.zeros([len(testing_data), max_sentence_len], dtype=np.int32)\n",
        "for i, sentence in enumerate(testing_data):\n",
        "  for t, word in enumerate(sentence):\n",
        "    x_test[i, t] = word2idx(word)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training word2vec...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
            "  if __name__ == '__main__':\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Result embedding shape: (85, 100)\n",
            "\n",
            "Preparing the data for LSTM...\n",
            "x_train shape: (15111, 890)\n",
            "x_test shape: (1679, 890)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic4mdmmvGb32",
        "outputId": "3c01655f-ecf9-464f-d7e0-68ddcc98d368",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print('y_train shape:', y_train.shape)\n",
        "print('y_test shape:', y_test.shape)\n",
        "\n",
        "print('vocab_size: {}'.format(vocab_size))\n",
        "print('emdedding_size: {}'.format(emdedding_size))\n",
        "\n",
        "review_length = max_sentence_len\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y_train shape: (15111,)\n",
            "y_test shape: (1679,)\n",
            "vocab_size: 85\n",
            "emdedding_size: 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jONFmMXrvFjO"
      },
      "source": [
        "## Create and build LSTM Recurrent Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pibIcL87vGqg",
        "outputId": "a0b1eb7e-2a25-4ec4-9cc8-65a3bfdd9279",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "# We begin by defining the a empty stack. We'll use this for building our \n",
        "# network, later by layer.\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "# The Embedding Layer provides a spatial mapping (or Word Embedding) of all the \n",
        "# individual words in our training set. Words close to one another share context \n",
        "# and or meaning. This spatial mapping is learning during the training process.\n",
        "\n",
        " \n",
        "model.add(\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim = vocab_size, # The size of our vocabulary \n",
        "        output_dim = emdedding_size, # Dimensions to which each words shall be mapped\n",
        "        weights=[pretrained_weights]\n",
        "    )\n",
        ")\n",
        " \n",
        "# Dropout layers fight overfitting and forces the model to learn multiple \n",
        "# representations of the same data by randomly disabling neurons in the \n",
        "# learning phase.\n",
        "model.add(\n",
        "    tf.keras.layers.Dropout(\n",
        "        rate=0.25 # Randomly disable 25% of neurons\n",
        "    )\n",
        ")\n",
        "\n",
        "# We are using a fast version of LSTM whih is optimised for GPUs. This layer \n",
        "# looks at the sequence of words in the review, along with their word embeddings\n",
        "# and uses both of these to determine to sentiment of a given review.\n",
        "model.add(\n",
        "    tf.compat.v1.keras.layers.CuDNNLSTM(\n",
        "        units=emdedding_size  \n",
        "    )\n",
        ")\n",
        "\n",
        "# Add a second dropout layer with the same aim as the first.\n",
        "model.add(\n",
        "    tf.keras.layers.Dropout(\n",
        "        rate=0.25 # Randomly disable 25% of neurons\n",
        "    )\n",
        ")\n",
        "\n",
        "# All LSTM units are connected to a single node in the dense layer. A sigmoid \n",
        "# activation function determines the output from this node - a value \n",
        "# between 0 and 1. Closer to 0 indicates a negative review. Closer to 1 \n",
        "# indicates a positive review.\n",
        "model.add(\n",
        "    tf.keras.layers.Dense(\n",
        "        units=vocab_size, # Single unit\n",
        "        activation='softmax' # Sigmoid activation function (output from 0 to 1)\n",
        "    )\n",
        ")\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    loss=tf.keras.losses.binary_crossentropy, # loss function\n",
        "    optimizer=tf.keras.optimizers.Adam(), # optimiser function\n",
        "    metrics=['accuracy']) # reporting metric\n",
        "\n",
        "# Display a summary of the models structure\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_9 (Embedding)      (None, None, 100)         8500      \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, None, 100)         0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm_9 (CuDNNLSTM)     (None, 100)               80800     \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 85)                8585      \n",
            "=================================================================\n",
            "Total params: 97,885\n",
            "Trainable params: 97,885\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvD185R-2_LZ",
        "outputId": "cba95753-1e21-4506-cdff-7247a7631932",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#x_train = np.array(x_train)\n",
        "#y_train = np.array(y_train)\n",
        "type(x_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBdg9IN56EHY",
        "outputId": "65e126f3-cead-4ebd-d636-8329b3a075aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC51lNHPwTcV"
      },
      "source": [
        "## Train the LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIn2JdwGwW0t",
        "outputId": "851f5c1c-1cb9-456e-939d-4ed243b593ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train the LSTM on the training data\n",
        "history = model.fit(\n",
        "\n",
        "    # Training data : features (review) and classes (positive or negative)\n",
        "    x_train, y_train,\n",
        "                    \n",
        "    # Number of samples to work through before updating the \n",
        "    # internal model parameters via back propagation. The \n",
        "    # higher the batch, the more memory you need.\n",
        "    batch_size=64, \n",
        "\n",
        "    # An epoch is an iteration over the entire training data.\n",
        "    epochs=50, \n",
        "    \n",
        "    # The model will set apart his fraction of the training \n",
        "    # data, will not train on it, and will evaluate the loss\n",
        "    # and any model metrics on this data at the end of \n",
        "    # each epoch.\n",
        "    validation_split=0.2,\n",
        "    \n",
        "    verbose=1\n",
        ") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "189/189 [==============================] - 10s 54ms/step - loss: 13.3055 - accuracy: 0.0157 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/50\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 13.3030 - accuracy: 0.0180 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/50\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 13.3029 - accuracy: 0.0168 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/50\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 13.3028 - accuracy: 0.0171 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/50\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 13.3028 - accuracy: 0.0175 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/50\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 13.3028 - accuracy: 0.0176 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/50\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 13.3028 - accuracy: 0.0160 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/50\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 13.3028 - accuracy: 0.0195 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/50\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 13.3028 - accuracy: 0.0186 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/50\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 13.3028 - accuracy: 0.0192 - val_loss: 13.2691 - val_accuracy: 0.1419\n",
            "Epoch 11/50\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 13.3028 - accuracy: 0.0189 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/50\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 13.3028 - accuracy: 0.0170 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/50\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 13.3028 - accuracy: 0.0180 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/50\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 13.3028 - accuracy: 0.0181 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/50\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 13.3028 - accuracy: 0.0190 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/50\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 13.3028 - accuracy: 0.0212 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/50\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 13.3028 - accuracy: 0.0183 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/50\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 13.3028 - accuracy: 0.0197 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 19/50\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 13.3028 - accuracy: 0.0202 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/50\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 13.3028 - accuracy: 0.0166 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 21/50\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 13.3028 - accuracy: 0.0163 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 22/50\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 13.3028 - accuracy: 0.0170 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 23/50\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 13.3028 - accuracy: 0.0195 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 24/50\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 13.3028 - accuracy: 0.0172 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 25/50\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 13.3028 - accuracy: 0.0168 - val_loss: 13.2691 - val_accuracy: 0.1370\n",
            "Epoch 26/50\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 13.3028 - accuracy: 0.0189 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 27/50\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 13.3028 - accuracy: 0.0172 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 28/50\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 13.3028 - accuracy: 0.0183 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 29/50\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 13.3028 - accuracy: 0.0194 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 30/50\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 13.3028 - accuracy: 0.0180 - val_loss: 13.2691 - val_accuracy: 0.1370\n",
            "Epoch 31/50\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 13.3028 - accuracy: 0.0188 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 32/50\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 13.3028 - accuracy: 0.0166 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 33/50\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 13.3028 - accuracy: 0.0186 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 34/50\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 13.3028 - accuracy: 0.0200 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 35/50\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 13.3028 - accuracy: 0.0158 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 36/50\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 13.3028 - accuracy: 0.0180 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 37/50\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 13.3028 - accuracy: 0.0170 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 38/50\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 13.3028 - accuracy: 0.0165 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 39/50\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 13.3028 - accuracy: 0.0148 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 40/50\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 13.3028 - accuracy: 0.0155 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 41/50\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 13.3028 - accuracy: 0.0165 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 42/50\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 13.3028 - accuracy: 0.0189 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 43/50\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 13.3028 - accuracy: 0.0161 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 44/50\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 13.3028 - accuracy: 0.0184 - val_loss: 13.2691 - val_accuracy: 0.1370\n",
            "Epoch 45/50\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 13.3028 - accuracy: 0.0176 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 46/50\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 13.3028 - accuracy: 0.0167 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 47/50\n",
            "189/189 [==============================] - 10s 53ms/step - loss: 13.3028 - accuracy: 0.0180 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 48/50\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 13.3028 - accuracy: 0.0160 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 49/50\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 13.3028 - accuracy: 0.0164 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n",
            "Epoch 50/50\n",
            "189/189 [==============================] - 10s 52ms/step - loss: 13.3028 - accuracy: 0.0179 - val_loss: 13.2691 - val_accuracy: 0.0000e+00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XqscrAg6fPF",
        "outputId": "39249697-c276-4f14-ce7d-7ac4b22fbe2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        }
      },
      "source": [
        "# Get Model Predictions for test data\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "predicted_classes = model.predict_classes(x_test)\n",
        "print(classification_report(y_test, predicted_classes, target_names=labels_index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-3b45dade9909>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredicted_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   1993\u001b[0m                 \u001b[0;34m\"Number of classes, {0}, does not match size of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m                 \u001b[0;34m\"target_names, {1}. Try specifying the labels \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1995\u001b[0;31m                 \u001b[0;34m\"parameter\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1996\u001b[0m             )\n\u001b[1;32m   1997\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Number of classes, 8, does not match size of target_names, 7. Try specifying the labels parameter"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T___U27_68nl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}