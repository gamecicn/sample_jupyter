{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Supervised NE Chunker.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gamecicn/sample_jupyter/blob/main/Supervised_NE_Chunker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9EMIvDz_Sjl"
      },
      "source": [
        "## Building a Supervised Classifier-based Chunker\n",
        "\n",
        "Regular-expression based chunkers and  n-gram chunkers decide what chunks to create entirely based on part-of-speech tags. \n",
        "\n",
        "In this exercise, we will leverage additional features to improve the unigram chunker."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHk_qeBh_Sjm"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import conll2000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jE-RztYm_Sjq"
      },
      "source": [
        "test_sents  = conll2000.chunked_sents('test.txt',  chunk_types=['NP'])\n",
        "train_sents = conll2000.chunked_sents('train.txt', chunk_types=['NP'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2JJvVrE_Sjs"
      },
      "source": [
        "---\n",
        "### Unigram Chunker\n",
        "The UnigramChunker class uses a unigram tagger to label sentences with chunk tags. The class defines two methods\n",
        "1. a constructor is called when we build a new UnigramChunker\n",
        "2. parse method is used to chunk new sentences.\n",
        "\n",
        "The goal here is to assign IOB tags to words in a sentence, and then convert those tags to chunks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llPohr0w_Sjt"
      },
      "source": [
        "class UnigramChunker(nltk.ChunkParserI):\n",
        "    def __init__(self, train_sents):\n",
        "        train_data = [[(t,c) for w,t,c in nltk.chunk.tree2conlltags(sent)]\n",
        "                      for sent in train_sents]\n",
        "        self.tagger = nltk.UnigramTagger(train_data)\n",
        "\n",
        "    def parse(self, sentence):\n",
        "        pos_tags = [pos for (word,pos) in sentence]\n",
        "        tagged_pos_tags = self.tagger.tag(pos_tags)\n",
        "        chunktags = [chunktag for (pos, chunktag) in tagged_pos_tags]\n",
        "        conlltags = [(word, pos, chunktag) for ((word,pos),chunktag)\n",
        "                     in zip(sentence, chunktags)]\n",
        "        return nltk.chunk.conlltags2tree(conlltags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpP1gC9Q_Sjv"
      },
      "source": [
        "unigram_chunker = UnigramChunker(train_sents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThUnz2gr_Sjy"
      },
      "source": [
        "print(unigram_chunker.evaluate(test_sents))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4wNRT5ba_Sj1"
      },
      "source": [
        "### A classifier-based chunker\n",
        "\n",
        "For the classifier-based chunker also assigns IOB tags to the words in a sentence, \n",
        "and then convert those tags to chunks.\n",
        "\n",
        "The below code defines two classes. \n",
        "- The first class calls a feature extractor and then uses a nlkt classifier method (e.g. Naive Bayes). \n",
        "- The second class is a wrapper around the tagger class that turns it into a chunker. \n",
        "  - During training, this second class maps the chunk trees in the training corpus into tag sequences; \n",
        "  - in the parse() method, it converts the tag sequence provided by the tagger back into a chunk tree."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUsLknni_Sj1"
      },
      "source": [
        "class ConsecutiveNPChunkTagger(nltk.TaggerI):\n",
        "    # this class inherits from base call nltk.TaggerI\n",
        "\n",
        "    def __init__(self, train_sents):\n",
        "        # train sents should be of form [[((w,t),c),...],[((w,t),c),...],...]\n",
        "        train_set = []\n",
        "        for tagged_sent in train_sents:\n",
        "            # given a tagged sentence, untag() returns an untagged version of the sentence\n",
        "            untagged_sent = nltk.tag.untag(tagged_sent)\n",
        "            \n",
        "            history = []\n",
        "            # we are going to iterate through the list of trained sentences\n",
        "            # and extract the index i along with the tag for each word\n",
        "            for i, (word, tag) in enumerate(tagged_sent):\n",
        "                # build an array of tuples (dict,label), where dict is dict of features\n",
        "                featureset = npchunk_features(untagged_sent, i, history)\n",
        "                train_set.append( (featureset, tag) )\n",
        "                history.append(tag)\n",
        "        #self.classifier = nltk.MaxentClassifier.train(train_set,trace=0)\n",
        "        self.classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "    def tag(self, sentence):\n",
        "        history = []\n",
        "        for i, word in enumerate(sentence):\n",
        "            featureset = npchunk_features(sentence, i, history)\n",
        "            tag = self.classifier.classify(featureset)\n",
        "            history.append(tag)\n",
        "        return zip(sentence, history)\n",
        "\n",
        "class ConsecutiveNPChunker(nltk.ChunkParserI):\n",
        "    def __init__(self, train_sents):\n",
        "        # given a NE-tagged sentence in tree format, convert to tag format and\n",
        "        # extract ((word,pos-tag),chunk-tag) tuples into an array\n",
        "        tagged_sents = [[((w,t),c) for (w,t,c) in\n",
        "                         nltk.chunk.tree2conlltags(sent)]\n",
        "                        for sent in train_sents]\n",
        "        # pass array of sentence arrays of tuples to tagger \n",
        "        self.tagger = ConsecutiveNPChunkTagger(tagged_sents)\n",
        "\n",
        "    def parse(self, sentence):\n",
        "        tagged_sents = self.tagger.tag(sentence)\n",
        "        conlltags = [(w,t,c) for ((w,t),c) in tagged_sents]\n",
        "        return nltk.chunk.conlltags2tree(conlltags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvbg-azx_Sj4"
      },
      "source": [
        "# this feature extractor just provides the part-of-speech tag of the current token\n",
        "# basically a unigram chunker\n",
        "def npchunk_features(sentence, i, history):\n",
        "    word, pos = sentence[i]\n",
        "    return {\"pos\": pos}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6eFwNcT_Sj7",
        "outputId": "8c3e814a-551c-4d0b-d63a-ebb3bee20436"
      },
      "source": [
        "chunker = ConsecutiveNPChunker(train_sents)\n",
        "print(chunker.evaluate(test_sents))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ChunkParse score:\n",
            "    IOB Accuracy:  92.9%%\n",
            "    Precision:     79.9%%\n",
            "    Recall:        86.8%%\n",
            "    F-Measure:     83.2%%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXK5mUtM_Sj_"
      },
      "source": [
        "sent = train_sents[123]\n",
        "sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An-lG6lx_SkB"
      },
      "source": [
        "tagged_sent = [((w,t),c) for (w,t,c) in nltk.chunk.tree2conlltags(sent)] \n",
        "tagged_sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zow_SLRj_SkE"
      },
      "source": [
        "untagged_sent = nltk.tag.untag(tagged_sent)\n",
        "untagged_sent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b1I4-jG_SkH"
      },
      "source": [
        "history = []\n",
        "train_set = []\n",
        "# we are going to iterate through the list of trained sentences\n",
        "# and extract the index i along with the tag for each word\n",
        "for i, (word, tag) in enumerate(tagged_sent):\n",
        "    featureset = npchunk_features(untagged_sent, i, history)\n",
        "    train_set.append( (featureset, tag) )\n",
        "    history.append(tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhhZnE1N_SkJ"
      },
      "source": [
        "# this feature extractor includes the part of speech for the previous word\n",
        "def npchunk_features(sentence, i, history):\n",
        "    word, pos = sentence[i]\n",
        "    if i == 0:\n",
        "        prevword, prevpos = \"<START>\", \"<START>\"\n",
        "    else:\n",
        "        prevword, prevpos = sentence[i-1]\n",
        "    return {\"pos\": pos, \"prevpos\": prevpos}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7L351fJg_SkL"
      },
      "source": [
        "def test_feature_extractor(tagged_sent):\n",
        "    history = []\n",
        "    train_set = []\n",
        "    # we are going to iterate through the list of trained sentences\n",
        "    # and extract the index i along with the tag for each word\n",
        "    for i, (word, tag) in enumerate(tagged_sent):\n",
        "        featureset = npchunk_features(untagged_sent, i, history)\n",
        "        train_set.append( (featureset, tag) )\n",
        "        history.append(tag)\n",
        "        \n",
        "    print(*train_set, sep='\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2j2a3Rt_SkN",
        "outputId": "ab559eaf-8451-4a4e-b341-772c729268b8"
      },
      "source": [
        "train_set"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[({'pos': 'CD', 'prevpos': '<START>'}, 'O'),\n",
              " ({'pos': '.', 'prevpos': 'CD'}, 'O'),\n",
              " ({'pos': 'RB', 'prevpos': '.'}, 'O'),\n",
              " ({'pos': 'CC', 'prevpos': 'RB'}, 'O'),\n",
              " ({'pos': 'RB', 'prevpos': 'CC'}, 'O'),\n",
              " ({'pos': ',', 'prevpos': 'RB'}, 'O'),\n",
              " ({'pos': 'NN', 'prevpos': ','}, 'O'),\n",
              " ({'pos': 'DT', 'prevpos': 'NN'}, 'B-NP'),\n",
              " ({'pos': 'PRP$', 'prevpos': 'DT'}, 'I-NP'),\n",
              " ({'pos': 'NNS', 'prevpos': 'PRP$'}, 'I-NP'),\n",
              " ({'pos': 'CC', 'prevpos': 'NNS'}, 'I-NP'),\n",
              " ({'pos': 'NNS', 'prevpos': 'CC'}, 'I-NP'),\n",
              " ({'pos': 'IN', 'prevpos': 'NNS'}, 'O'),\n",
              " ({'pos': 'DT', 'prevpos': 'IN'}, 'B-NP'),\n",
              " ({'pos': 'NN', 'prevpos': 'DT'}, 'I-NP'),\n",
              " ({'pos': 'POS', 'prevpos': 'NN'}, 'B-NP'),\n",
              " ({'pos': 'NN', 'prevpos': 'POS'}, 'I-NP'),\n",
              " ({'pos': 'NN', 'prevpos': 'NN'}, 'I-NP'),\n",
              " ({'pos': '.', 'prevpos': 'NN'}, 'O')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAv8dD_t_SkQ",
        "outputId": "46136b1f-3fab-4d3a-b308-2d90e7d7ddf0"
      },
      "source": [
        "chunker = ConsecutiveNPChunker(train_sents)\n",
        "print(chunker.evaluate(test_sents))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ChunkParse score:\n",
            "    IOB Accuracy:  93.6%%\n",
            "    Precision:     81.9%%\n",
            "    Recall:        88.6%%\n",
            "    F-Measure:     85.1%%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z0NgQfc_SkS"
      },
      "source": [
        "def npchunk_features(sentence, i, history):\n",
        "    word, pos = sentence[i]\n",
        "    if i == 0:\n",
        "        prevword, prevpos = \"<START>\", \"<START>\"\n",
        "    else:\n",
        "        prevword, prevpos = sentence[i-1]\n",
        "    return {\"pos\": pos, \"word\": word, \"prevpos\": prevpos}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYWPDcZ__SkU",
        "outputId": "11831c46-3129-4dce-9eb4-d83aa6bad8dc"
      },
      "source": [
        "test_feature_extractor(tagged_sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "({'pos': 'CD', 'word': '2', 'prevpos': '<START>'}, 'O')\n",
            "({'pos': '.', 'word': '.', 'prevpos': 'CD'}, 'O')\n",
            "({'pos': 'RB', 'word': 'Formally', 'prevpos': '.'}, 'O')\n",
            "({'pos': 'CC', 'word': 'or', 'prevpos': 'RB'}, 'O')\n",
            "({'pos': 'RB', 'word': 'informally', 'prevpos': 'CC'}, 'O')\n",
            "({'pos': ',', 'word': ',', 'prevpos': 'RB'}, 'O')\n",
            "({'pos': 'NN', 'word': 'train', 'prevpos': ','}, 'O')\n",
            "({'pos': 'DT', 'word': 'all', 'prevpos': 'NN'}, 'B-NP')\n",
            "({'pos': 'PRP$', 'word': 'your', 'prevpos': 'DT'}, 'I-NP')\n",
            "({'pos': 'NNS', 'word': 'managers', 'prevpos': 'PRP$'}, 'I-NP')\n",
            "({'pos': 'CC', 'word': 'and', 'prevpos': 'NNS'}, 'I-NP')\n",
            "({'pos': 'NNS', 'word': 'supervisors', 'prevpos': 'CC'}, 'I-NP')\n",
            "({'pos': 'IN', 'word': 'in', 'prevpos': 'NNS'}, 'O')\n",
            "({'pos': 'DT', 'word': 'the', 'prevpos': 'IN'}, 'B-NP')\n",
            "({'pos': 'NN', 'word': 'company', 'prevpos': 'DT'}, 'I-NP')\n",
            "({'pos': 'POS', 'word': \"'s\", 'prevpos': 'NN'}, 'B-NP')\n",
            "({'pos': 'NN', 'word': 'due-process', 'prevpos': 'POS'}, 'I-NP')\n",
            "({'pos': 'NN', 'word': 'approach', 'prevpos': 'NN'}, 'I-NP')\n",
            "({'pos': '.', 'word': '.', 'prevpos': 'NN'}, 'O')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EADqAuvD_SkX",
        "outputId": "9b99189f-f3c3-44bd-e27d-9a9c04224c93"
      },
      "source": [
        "chunker = ConsecutiveNPChunker(train_sents)\n",
        "print(chunker.evaluate(test_sents))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ChunkParse score:\n",
            "    IOB Accuracy:  94.4%%\n",
            "    Precision:     84.1%%\n",
            "    Recall:        89.8%%\n",
            "    F-Measure:     86.9%%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2goZBLY_SkZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}